{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bigya01/cerviscan/blob/main/Cervix%20Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRG0BkKWKdNy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ9vDwxLMr2w"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRKj8rlXK0Cv"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  * loaded images from the file path\n",
        "  * splitted data into 3 parts : train, test, val\n",
        "  * removed corrupted pictures, resized to (180,180)\n",
        "  * normalized pictures by dividing each picture by 255 and dumped it to pickle cause my notebook couldn't take it\n",
        "  * augmented params of pics like height weight shear zoom etc to introduce diversity\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import pickle\n",
        "train_features,train_labels =None,None\n",
        "with open('/content/drive/MyDrive/cervix/train.pickle','rb') as handle:\n",
        "    train_features,train_labels = pickle.load(handle)\n",
        "val_features, val_labels = None, None\n",
        "with open('/content/drive/MyDrive/cervix/val.pickle','rb') as handle:\n",
        "    val_features,val_labels = pickle.load(handle)\n",
        "test_features, test_labels = None, None\n",
        "with open('/content/drive/MyDrive/cervix/test.pickle', 'rb') as handle:\n",
        "    test_features, test_labels = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8jhnhutAOOow"
      },
      "outputs": [],
      "source": [
        "# val_features, val_labels = None, None\n",
        "# with open('/content/drive/MyDrive/cervix/val.pickle','rb') as handle:\n",
        "#     val_features,val_labels = pickle.load(handle)\n",
        "# test_features, test_labels = None, None\n",
        "# with open('/content/drive/MyDrive/cervix/test.pickle', 'rb') as handle:\n",
        "#     test_features, test_labels = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4nwWqmcmMMe1"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/cervix/X_train.pickle', 'wb') as f:\n",
        "#     pickle.dump(X_train, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DAmJ8LPQNRqH",
        "outputId": "26b0e022-247b-4ff2-e2e6-b10d741ffeb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6569, 180, 180, 3)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train = None\n",
        "with open('/content/drive/MyDrive/cervix/X_train.pickle','rb') as handle:\n",
        "    X_train = pickle.load(handle)\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lZ6ccuteNo70",
        "outputId": "35ad73d5-d61d-4ded-837c-90c0201f20a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(821, 180, 180, 3)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_val = None\n",
        "with open('/content/drive/MyDrive/cervix/X_val.pickle','rb') as handle:\n",
        "    X_val = pickle.load(handle)\n",
        "X_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C8IfN0kRNzk7",
        "outputId": "b7b13551-3a82-46bb-babd-720b18cda107"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(822, 180, 180, 3)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test = None\n",
        "with open('/content/drive/MyDrive/cervix/X_test.pickle','rb') as handle:\n",
        "    X_test = pickle.load(handle)\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H7ff_AdZOA2D"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# creates a mapping s.t type1 assigned 0, type2 1,type3 2\n",
        "le = LabelEncoder().fit(['Type 1','Type 2','Type 3'])\n",
        "y_train = le.transform(train_labels)\n",
        "y_val = le.transform(val_labels)\n",
        "y_test = le.transform(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oTH1c-POOebK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def add_noise(img):\n",
        "    VARIABILITY = 50  #extent of intensity of noise to be added\n",
        "    deviation = VARIABILITY*random.random()  #magnitude of random noise added to img\n",
        "    #noise with normal distr with magnitude of deviation and noise shape is same as image shape\n",
        "    noise = np.random.normal(0,deviation,img.shape)\n",
        "    img += noise\n",
        "    np.clip(img,0.,255.) #anything below 0 is 0 and above 255 is 255\n",
        "    return img\n",
        "\n",
        "train_datagen = ImageDataGenerator(   #so train data sees all kind of diverse data\n",
        "    rotation_range  = 40,\n",
        "    zoom_range = 0.2,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True\n",
        ")\n",
        "eval_datagen = ImageDataGenerator()  # no augmentation params cause we wanna evaluate model's performance on original dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G4DEGfeyOnp3"
      },
      "outputs": [],
      "source": [
        "#augmenting features\n",
        "BATCH_SIZE = 6\n",
        "train_gen = train_datagen.flow(X_train,y_train,batch_size = BATCH_SIZE)\n",
        "val_gen = eval_datagen.flow(X_val,y_val,batch_size = BATCH_SIZE)\n",
        "test_gen = eval_datagen.flow(X_val,y_val,batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i2H0A5XIO6GQ",
        "outputId": "c2596b67-ecdf-47b9-935d-3ff9d1f6fec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data batch shape:(6, 180, 180, 3)\n",
            " labels batch shape: (6,)\n"
          ]
        }
      ],
      "source": [
        "for data_batch, labels_batch in train_gen:\n",
        "  print('data batch shape:{}\\n labels batch shape: {}'.format(data_batch.shape,labels_batch.shape))\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k8UJwXpPTc_V",
        "outputId": "4b98c80b-af31-4ae0-9611-cdcc0d5162c1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAGsCAYAAABjIvUkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoJUlEQVR4nO3dfXBUVYL38V8IdPPanQmQdFIERNkBggQER+gaZUCYBAgMrlC7DEii8rJQwS2ICzFVFDK4axBUxBGhZl0Js5IxuCvumAzBEAyoBNAMEQiaHVjcMBs6YWGSBoQQkvv88Ty5j62A9KFDAn4/Vbcq3ff07XOsi/lW9+1OmGVZlgAAAILUrrUnAAAAbk9EBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADBCRAAAACPtW3sCLaWpqUlVVVXq1q2bwsLCWns6AADcNizL0rlz5xQbG6t27a79esMdGxFVVVWKi4tr7WkAAHDbOnnypHr16nXN/XdsRHTr1k3S//0P4HK5Wnk2AADcPvx+v+Li4uzfpddyx0ZE81sYLpeLiAAAwMD3XQ7AhZUAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwEhQEbFhwwYlJCTY3wLp9Xq1fft2e//o0aMVFhYWsM2fPz/gGJWVlUpOTlbnzp0VFRWlJUuW6MqVKwFjiouLNWzYMDmdTvXr10/Z2dnmKwQAAC0iqK+97tWrl1atWqW/+qu/kmVZ2rx5s6ZMmaKDBw9q0KBBkqS5c+dq5cqV9mM6d+5s/9zY2Kjk5GR5PB7t3btXp06dUkpKijp06KDnn39eknTixAklJydr/vz52rJli4qKijRnzhzFxMQoKSkpFGsGAAAhEGZZlnUzB4iMjNSaNWs0e/ZsjR49WkOHDtUrr7xy1bHbt2/XpEmTVFVVpejoaEnSxo0blZGRodOnT8vhcCgjI0P5+fk6cuSI/bjp06ertrZWBQUFNzwvv98vt9uturo6/nYGAABBuNHfocbXRDQ2Nurtt9/WhQsX5PV67fu3bNmiHj166N5771VmZqa+/vpre19JSYkGDx5sB4QkJSUlye/3q7y83B4zbty4gOdKSkpSSUnJdedTX18vv98fsAEAgJYT9F/xPHz4sLxery5duqSuXbtq27Ztio+PlyTNmDFDffr0UWxsrA4dOqSMjAxVVFTo3XfflST5fL6AgJBk3/b5fNcd4/f7dfHiRXXq1Omq88rKytKvfvWrYJcDAAAMBR0R/fv3V1lZmerq6vRv//ZvSk1N1e7duxUfH6958+bZ4wYPHqyYmBiNHTtWx48f1z333BPSiX9bZmam0tPT7dvNfwsdAAC0jKDfznA4HOrXr5+GDx+urKwsDRkyROvWrbvq2BEjRkiSjh07JknyeDyqrq4OGNN82+PxXHeMy+W65qsQkuR0Ou1PjTRvAACg5QT9SsS3NTU1qb6+/qr7ysrKJEkxMTGSJK/Xq3/6p39STU2NoqKiJEmFhYVyuVz2WyJer1d/+MMfAo5TWFgYcN0FcCe665n81p4CWthXq5JbewpASAUVEZmZmZowYYJ69+6tc+fOKScnR8XFxdqxY4eOHz+unJwcTZw4Ud27d9ehQ4e0ePFijRo1SgkJCZKkxMRExcfHa9asWVq9erV8Pp+WLVumtLQ0OZ1OSdL8+fP12muvaenSpXryySe1a9cubd26Vfn5/A8WAIC2JKiIqKmpUUpKik6dOiW3262EhATt2LFDP//5z3Xy5Ent3LlTr7zyii5cuKC4uDhNnTpVy5Ytsx8fHh6uvLw8LViwQF6vV126dFFqamrA90r07dtX+fn5Wrx4sdatW6devXrpjTfe4DsiAABoY276eyLaKr4nArcb3s648/F2Bm4XLf49EQAA4IeNiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGgoqIDRs2KCEhQS6XSy6XS16vV9u3b7f3X7p0SWlpaerevbu6du2qqVOnqrq6OuAYlZWVSk5OVufOnRUVFaUlS5boypUrAWOKi4s1bNgwOZ1O9evXT9nZ2eYrBAAALSKoiOjVq5dWrVql0tJSffbZZ3r44Yc1ZcoUlZeXS5IWL16s999/X++88452796tqqoqPfroo/bjGxsblZycrMuXL2vv3r3avHmzsrOztXz5cnvMiRMnlJycrDFjxqisrEyLFi3SnDlztGPHjhAtGQAAhEKYZVnWzRwgMjJSa9as0bRp09SzZ0/l5ORo2rRpkqQvv/xSAwcOVElJiUaOHKnt27dr0qRJqqqqUnR0tCRp48aNysjI0OnTp+VwOJSRkaH8/HwdOXLEfo7p06ertrZWBQUFNzwvv98vt9uturo6uVyum1kicEvc9Ux+a08BLeyrVcmtPQXghtzo71DjayIaGxv19ttv68KFC/J6vSotLVVDQ4PGjRtnjxkwYIB69+6tkpISSVJJSYkGDx5sB4QkJSUlye/3269mlJSUBByjeUzzMa6lvr5efr8/YAMAAC0n6Ig4fPiwunbtKqfTqfnz52vbtm2Kj4+Xz+eTw+FQREREwPjo6Gj5fD5Jks/nCwiI5v3N+643xu/36+LFi9ecV1ZWltxut73FxcUFuzQAABCEoCOif//+Kisr0/79+7VgwQKlpqbq6NGjLTG3oGRmZqqurs7eTp482dpTAgDgjtY+2Ac4HA7169dPkjR8+HB9+umnWrdunf72b/9Wly9fVm1tbcCrEdXV1fJ4PJIkj8ejAwcOBByv+dMb3xzz7U90VFdXy+VyqVOnTtecl9PplNPpDHY5AADA0E1/T0RTU5Pq6+s1fPhwdejQQUVFRfa+iooKVVZWyuv1SpK8Xq8OHz6smpoae0xhYaFcLpfi4+PtMd88RvOY5mMAAIC2IahXIjIzMzVhwgT17t1b586dU05OjoqLi7Vjxw653W7Nnj1b6enpioyMlMvl0lNPPSWv16uRI0dKkhITExUfH69Zs2Zp9erV8vl8WrZsmdLS0uxXEebPn6/XXntNS5cu1ZNPPqldu3Zp69atys/nynUAANqSoCKipqZGKSkpOnXqlNxutxISErRjxw79/Oc/lyStXbtW7dq109SpU1VfX6+kpCS9/vrr9uPDw8OVl5enBQsWyOv1qkuXLkpNTdXKlSvtMX379lV+fr4WL16sdevWqVevXnrjjTeUlJQUoiUDAIBQuOnviWir+J4I3G74nog7H98TgdtFi39PBAAA+GEjIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAICRoCIiKytLP/nJT9StWzdFRUXpkUceUUVFRcCY0aNHKywsLGCbP39+wJjKykolJyerc+fOioqK0pIlS3TlypWAMcXFxRo2bJicTqf69eun7OxssxUCAIAWEVRE7N69W2lpadq3b58KCwvV0NCgxMREXbhwIWDc3LlzderUKXtbvXq1va+xsVHJycm6fPmy9u7dq82bNys7O1vLly+3x5w4cULJyckaM2aMysrKtGjRIs2ZM0c7duy4yeUCAIBQaR/M4IKCgoDb2dnZioqKUmlpqUaNGmXf37lzZ3k8nqse44MPPtDRo0e1c+dORUdHa+jQoXruueeUkZGhFStWyOFwaOPGjerbt69eeuklSdLAgQP18ccfa+3atUpKSgp2jQAAoAXc1DURdXV1kqTIyMiA+7ds2aIePXro3nvvVWZmpr7++mt7X0lJiQYPHqzo6Gj7vqSkJPn9fpWXl9tjxo0bF3DMpKQklZSUXHMu9fX18vv9ARsAAGg5Qb0S8U1NTU1atGiRfvrTn+ree++1758xY4b69Omj2NhYHTp0SBkZGaqoqNC7774rSfL5fAEBIcm+7fP5rjvG7/fr4sWL6tSp03fmk5WVpV/96lemywEAAEEyjoi0tDQdOXJEH3/8ccD98+bNs38ePHiwYmJiNHbsWB0/flz33HOP+Uy/R2ZmptLT0+3bfr9fcXFxLfZ8AAD80Bm9nbFw4ULl5eXpww8/VK9eva47dsSIEZKkY8eOSZI8Ho+qq6sDxjTfbr6O4lpjXC7XVV+FkCSn0ymXyxWwAQCAlhNURFiWpYULF2rbtm3atWuX+vbt+72PKSsrkyTFxMRIkrxerw4fPqyamhp7TGFhoVwul+Lj4+0xRUVFAccpLCyU1+sNZroAAKAFBRURaWlpeuutt5STk6Nu3brJ5/PJ5/Pp4sWLkqTjx4/rueeeU2lpqb766iv9/ve/V0pKikaNGqWEhARJUmJiouLj4zVr1ix9/vnn2rFjh5YtW6a0tDQ5nU5J0vz58/Vf//VfWrp0qb788ku9/vrr2rp1qxYvXhzi5QMAAFNBRcSGDRtUV1en0aNHKyYmxt5yc3MlSQ6HQzt37lRiYqIGDBigp59+WlOnTtX7779vHyM8PFx5eXkKDw+X1+vVY489ppSUFK1cudIe07dvX+Xn56uwsFBDhgzRSy+9pDfeeIOPdwIA0IaEWZZltfYkWoLf75fb7VZdXR3XR+C2cNcz+a09BbSwr1Ylt/YUgBtyo79D+dsZAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjAQVEVlZWfrJT36ibt26KSoqSo888ogqKioCxly6dElpaWnq3r27unbtqqlTp6q6ujpgTGVlpZKTk9W5c2dFRUVpyZIlunLlSsCY4uJiDRs2TE6nU/369VN2drbZCgEAQIsIKiJ2796ttLQ07du3T4WFhWpoaFBiYqIuXLhgj1m8eLHef/99vfPOO9q9e7eqqqr06KOP2vsbGxuVnJysy5cva+/evdq8ebOys7O1fPlye8yJEyeUnJysMWPGqKysTIsWLdKcOXO0Y8eOECwZAACEQphlWZbpg0+fPq2oqCjt3r1bo0aNUl1dnXr27KmcnBxNmzZNkvTll19q4MCBKikp0ciRI7V9+3ZNmjRJVVVVio6OliRt3LhRGRkZOn36tBwOhzIyMpSfn68jR47YzzV9+nTV1taqoKDgqnOpr69XfX29fdvv9ysuLk51dXVyuVymSwRumbueyW/tKaCFfbUqubWnANwQv98vt9v9vb9Db+qaiLq6OklSZGSkJKm0tFQNDQ0aN26cPWbAgAHq3bu3SkpKJEklJSUaPHiwHRCSlJSUJL/fr/LycnvMN4/RPKb5GFeTlZUlt9ttb3FxcTezNAAA8D2MI6KpqUmLFi3ST3/6U917772SJJ/PJ4fDoYiIiICx0dHR8vl89phvBkTz/uZ91xvj9/t18eLFq84nMzNTdXV19nby5EnTpQEAgBvQ3vSBaWlpOnLkiD7++ONQzseY0+mU0+ls7WkAAPCDYfRKxMKFC5WXl6cPP/xQvXr1su/3eDy6fPmyamtrA8ZXV1fL4/HYY779aY3m2983xuVyqVOnTiZTBgAAIRZURFiWpYULF2rbtm3atWuX+vbtG7B/+PDh6tChg4qKiuz7KioqVFlZKa/XK0nyer06fPiwampq7DGFhYVyuVyKj4+3x3zzGM1jmo8BAABaX1BvZ6SlpSknJ0f/8R//oW7dutnXMLjdbnXq1Elut1uzZ89Wenq6IiMj5XK59NRTT8nr9WrkyJGSpMTERMXHx2vWrFlavXq1fD6fli1bprS0NPvtiPnz5+u1117T0qVL9eSTT2rXrl3aunWr8vO5eh0AgLYiqFciNmzYoLq6Oo0ePVoxMTH2lpuba49Zu3atJk2apKlTp2rUqFHyeDx699137f3h4eHKy8tTeHi4vF6vHnvsMaWkpGjlypX2mL59+yo/P1+FhYUaMmSIXnrpJb3xxhtKSkoKwZIBAEAo3NT3RLRlN/oZV6Ct4Hsi7nyt9T0RnFt3vlCfW7fkeyIAAMAPFxEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwAgRAQAAjBARAADACBEBAACMEBEAAMAIEQEAAIwQEQAAwEjQEbFnzx5NnjxZsbGxCgsL03vvvRew//HHH1dYWFjANn78+IAxZ8+e1cyZM+VyuRQREaHZs2fr/PnzAWMOHTqkhx56SB07dlRcXJxWr14d/OoAAECLCToiLly4oCFDhmj9+vXXHDN+/HidOnXK3n73u98F7J85c6bKy8tVWFiovLw87dmzR/PmzbP3+/1+JSYmqk+fPiotLdWaNWu0YsUK/eY3vwl2ugAAoIW0D/YBEyZM0IQJE647xul0yuPxXHXfF198oYKCAn366ae6//77JUm//vWvNXHiRL344ouKjY3Vli1bdPnyZb355ptyOBwaNGiQysrK9PLLLwfEBgAAaD0tck1EcXGxoqKi1L9/fy1YsEBnzpyx95WUlCgiIsIOCEkaN26c2rVrp/3799tjRo0aJYfDYY9JSkpSRUWF/vKXv1z1Oevr6+X3+wM2AADQckIeEePHj9dvf/tbFRUV6YUXXtDu3bs1YcIENTY2SpJ8Pp+ioqICHtO+fXtFRkbK5/PZY6KjowPGNN9uHvNtWVlZcrvd9hYXFxfqpQEAgG8I+u2M7zN9+nT758GDByshIUH33HOPiouLNXbs2FA/nS0zM1Pp6en2bb/fT0gAANCCWvwjnnfffbd69OihY8eOSZI8Ho9qamoCxly5ckVnz561r6PweDyqrq4OGNN8+1rXWjidTrlcroANAAC0nBaPiD//+c86c+aMYmJiJEler1e1tbUqLS21x+zatUtNTU0aMWKEPWbPnj1qaGiwxxQWFqp///760Y9+1NJTBgAANyDoiDh//rzKyspUVlYmSTpx4oTKyspUWVmp8+fPa8mSJdq3b5+++uorFRUVacqUKerXr5+SkpIkSQMHDtT48eM1d+5cHThwQJ988okWLlyo6dOnKzY2VpI0Y8YMORwOzZ49W+Xl5crNzdW6desC3q4AAACtK+iI+Oyzz3TffffpvvvukySlp6frvvvu0/LlyxUeHq5Dhw7pF7/4hX784x9r9uzZGj58uD766CM5nU77GFu2bNGAAQM0duxYTZw4UQ8++GDAd0C43W598MEHOnHihIYPH66nn35ay5cv5+OdAAC0IUFfWDl69GhZlnXN/Tt27PjeY0RGRionJ+e6YxISEvTRRx8FOz0AAHCL8LczAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGGnf2hO43dz1TH5rTwEt7KtVya09BQC4LfBKBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADBCRAAAACNEBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADASdETs2bNHkydPVmxsrMLCwvTee+8F7LcsS8uXL1dMTIw6deqkcePG6U9/+lPAmLNnz2rmzJlyuVyKiIjQ7Nmzdf78+YAxhw4d0kMPPaSOHTsqLi5Oq1evDn51AACgxQQdERcuXNCQIUO0fv36q+5fvXq1Xn31VW3cuFH79+9Xly5dlJSUpEuXLtljZs6cqfLychUWFiovL0979uzRvHnz7P1+v1+JiYnq06ePSktLtWbNGq1YsUK/+c1vDJYIAABaQtB/xXPChAmaMGHCVfdZlqVXXnlFy5Yt05QpUyRJv/3tbxUdHa333ntP06dP1xdffKGCggJ9+umnuv/++yVJv/71rzVx4kS9+OKLio2N1ZYtW3T58mW9+eabcjgcGjRokMrKyvTyyy8HxAYAAGg9Ib0m4sSJE/L5fBo3bpx9n9vt1ogRI1RSUiJJKikpUUREhB0QkjRu3Di1a9dO+/fvt8eMGjVKDofDHpOUlKSKigr95S9/uepz19fXy+/3B2wAAKDlhDQifD6fJCk6Ojrg/ujoaHufz+dTVFRUwP727dsrMjIyYMzVjvHN5/i2rKwsud1ue4uLi7v5BQEAgGu6Yz6dkZmZqbq6Ons7efJka08JAIA7WkgjwuPxSJKqq6sD7q+urrb3eTwe1dTUBOy/cuWKzp49GzDmasf45nN8m9PplMvlCtgAAEDLCWlE9O3bVx6PR0VFRfZ9fr9f+/fvl9frlSR5vV7V1taqtLTUHrNr1y41NTVpxIgR9pg9e/aooaHBHlNYWKj+/fvrRz/6USinDAAADAUdEefPn1dZWZnKysok/d+LKcvKylRZWamwsDAtWrRI//iP/6jf//73Onz4sFJSUhQbG6tHHnlEkjRw4ECNHz9ec+fO1YEDB/TJJ59o4cKFmj59umJjYyVJM2bMkMPh0OzZs1VeXq7c3FytW7dO6enpIVs4AAC4OUF/xPOzzz7TmDFj7NvNv9hTU1OVnZ2tpUuX6sKFC5o3b55qa2v14IMPqqCgQB07drQfs2XLFi1cuFBjx45Vu3btNHXqVL366qv2frfbrQ8++EBpaWkaPny4evTooeXLl/PxTgAA2pCgI2L06NGyLOua+8PCwrRy5UqtXLnymmMiIyOVk5Nz3edJSEjQRx99FOz0AADALXLHfDoDAADcWkQEAAAwQkQAAAAjRAQAADBCRAAAACNEBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADBCRAAAACNEBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADBCRAAAACNEBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADBCRAAAACNEBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADBCRAAAACNEBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADBCRAAAACNEBAAAMEJEAAAAI0QEAAAwQkQAAAAjRAQAADBCRAAAACNEBAAAMBLyiFixYoXCwsICtgEDBtj7L126pLS0NHXv3l1du3bV1KlTVV1dHXCMyspKJScnq3PnzoqKitKSJUt05cqVUE8VAADchPYtcdBBgwZp586d//9J2v//p1m8eLHy8/P1zjvvyO12a+HChXr00Uf1ySefSJIaGxuVnJwsj8ejvXv36tSpU0pJSVGHDh30/PPPt8R0AQCAgRaJiPbt28vj8Xzn/rq6Ov3Lv/yLcnJy9PDDD0uSNm3apIEDB2rfvn0aOXKkPvjgAx09elQ7d+5UdHS0hg4dqueee04ZGRlasWKFHA5HS0wZAAAEqUWuifjTn/6k2NhY3X333Zo5c6YqKyslSaWlpWpoaNC4cePssQMGDFDv3r1VUlIiSSopKdHgwYMVHR1tj0lKSpLf71d5efk1n7O+vl5+vz9gAwAALSfkETFixAhlZ2eroKBAGzZs0IkTJ/TQQw/p3Llz8vl8cjgcioiICHhMdHS0fD6fJMnn8wUERPP+5n3XkpWVJbfbbW9xcXGhXRgAAAgQ8rczJkyYYP+ckJCgESNGqE+fPtq6das6deoU6qezZWZmKj093b7t9/sJCQAAWlCLf8QzIiJCP/7xj3Xs2DF5PB5dvnxZtbW1AWOqq6vtayg8Hs93Pq3RfPtq11k0czqdcrlcARsAAGg5LR4R58+f1/HjxxUTE6Phw4erQ4cOKioqsvdXVFSosrJSXq9XkuT1enX48GHV1NTYYwoLC+VyuRQfH9/S0wUAADco5G9n/MM//IMmT56sPn36qKqqSs8++6zCw8P1y1/+Um63W7Nnz1Z6eroiIyPlcrn01FNPyev1auTIkZKkxMRExcfHa9asWVq9erV8Pp+WLVumtLQ0OZ3OUE8XAAAYCnlE/PnPf9Yvf/lLnTlzRj179tSDDz6offv2qWfPnpKktWvXql27dpo6darq6+uVlJSk119/3X58eHi48vLytGDBAnm9XnXp0kWpqalauXJlqKcKAABuQsgj4u23377u/o4dO2r9+vVav379Ncf06dNHf/jDH0I9NQAAEEL87QwAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABGiAgAAGCEiAAAAEaICAAAYISIAAAARogIAABghIgAAABG2nRErF+/XnfddZc6duyoESNG6MCBA609JQAA8P+02YjIzc1Venq6nn32Wf3xj3/UkCFDlJSUpJqamtaeGgAAkNS+tSdwLS+//LLmzp2rJ554QpK0ceNG5efn680339QzzzzznfH19fWqr6+3b9fV1UmS/H5/SOfVVP91SI+HtifU58yN4ty683FuoaWE+txqPp5lWdcfaLVB9fX1Vnh4uLVt27aA+1NSUqxf/OIXV33Ms88+a0liY2NjY2NjC9F28uTJ6/6+bpOvRPzv//6vGhsbFR0dHXB/dHS0vvzyy6s+JjMzU+np6fbtpqYmnT17Vt27d1dYWFiLzvdO5ff7FRcXp5MnT8rlcrX2dHAH4dxCS+HcCg3LsnTu3DnFxsZed1ybjAgTTqdTTqcz4L6IiIjWmcwdxuVy8Y8RLYJzCy2Fc+vmud3u7x3TJi+s7NGjh8LDw1VdXR1wf3V1tTweTyvNCgAAfFObjAiHw6Hhw4erqKjIvq+pqUlFRUXyer2tODMAANCszb6dkZ6ertTUVN1///164IEH9Morr+jChQv2pzXQ8pxOp5599tnvvE0E3CzOLbQUzq1bK8yyvu/zG63ntdde05o1a+Tz+TR06FC9+uqrGjFiRGtPCwAAqI1HBAAAaLva5DURAACg7SMiAACAESICAAAYISIAAIARIuIOERYWdt1txYoVt3xOly5d0uOPP67Bgwerffv2euSRR275HHDz2uK5VVxcrClTpigmJkZdunTR0KFDtWXLlls+D9yctnhuVVRUaMyYMYqOjlbHjh119913a9myZWpoaLjlc7kdtNnviUBwTp06Zf+cm5ur5cuXq6Kiwr6va9eut3xOjY2N6tSpk/7+7/9e//7v/37Lnx+h0RbPrb179yohIUEZGRmKjo5WXl6eUlJS5Ha7NWnSpFs+H5hpi+dWhw4dlJKSomHDhikiIkKff/655s6dq6amJj3//PO3fD5tXgj+6CbamE2bNllut9uyLMs6f/681a1bN+udd94JGLNt2zarc+fOlt/vt06cOGFJsn73u99ZXq/Xcjqd1qBBg6zi4uKAxxw+fNgaP3681aVLFysqKsp67LHHrNOnT9/QnFJTU60pU6aEYnloRW3x3Go2ceJE64knnrip9aH1tOVza/HixdaDDz54U+u7U/F2xh2uS5cumj59ujZt2hRw/6ZNmzRt2jR169bNvm/JkiV6+umndfDgQXm9Xk2ePFlnzpyRJNXW1urhhx/Wfffdp88++0wFBQWqrq7W3/zN39zS9aDtaGvnVl1dnSIjI29+YWh1bencOnbsmAoKCvSzn/0sNIu707R2xSD0vln0lmVZ+/fvt8LDw62qqirLsiyrurraat++vV3szUW/atUq+zENDQ1Wr169rBdeeMGyLMt67rnnrMTExIDnOXnypCXJqqio+N458UrEnaEtnluWZVm5ubmWw+Gwjhw5cjPLQytqa+dW86sbkqx58+ZZjY2NoVjmHYdXIn4AHnjgAQ0aNEibN2+WJL311lvq06ePRo0aFTDum3/crH379rr//vv1xRdfSJI+//xzffjhh+ratau9DRgwQJJ0/PjxW7QStDVt4dz68MMP9cQTT+if//mfNWjQoFAtDa2stc+t3Nxc/fGPf1ROTo7y8/P14osvhnJ5dwwurPyBmDNnjtavX69nnnlGmzZt0hNPPKGwsLAbfvz58+c1efJkvfDCC9/ZFxMTE8qp4jbTmufW7t27NXnyZK1du1YpKSlBzx1tW2ueW3FxcZKk+Ph4NTY2at68eXr66acVHh4e3CLucLwS8QPx2GOP6b//+7/16quv6ujRo0pNTf3OmH379tk/X7lyRaWlpRo4cKAkadiwYSovL9ddd92lfv36BWxdunS5ZetA29Na51ZxcbGSk5P1wgsvaN68eaFfGFpdW/n/VlNTkxoaGtTU1HTzi7rTtPb7KQi9b7+32GzGjBmWw+Gwxo8fH3B/83uLvXv3tt59913riy++sObNm2d17drVvor5f/7nf6yePXta06ZNsw4cOGAdO3bMKigosB5//HHrypUr15xLeXm5dfDgQWvy5MnW6NGjrYMHD1oHDx4M5XJxC7WVc2vXrl1W586drczMTOvUqVP2dubMmZCvGbdGWzm33nrrLSs3N9c6evSodfz4cSs3N9eKjY21Zs6cGfI13wmIiDvQtf4xFhUVWZKsrVu3Btzf/I8xJyfHeuCBByyHw2HFx8dbu3btChj3n//5n9Zf//VfWxEREVanTp2sAQMGWIsWLbKampquOZc+ffpYkr6z4fbUVs6t1NTUq55XP/vZz0K1VNxibeXcevvtt61hw4ZZXbt2tbp06WLFx8dbzz//vHXx4sWQrfVOwp8C/wH513/9Vy1evFhVVVVyOBz2/V999ZX69u2rgwcPaujQoa03Qdy2OLfQUji32jYurPwB+Prrr3Xq1CmtWrVKf/d3fxfwDxG4GZxbaCmcW7cHLqz8AVi9erUGDBggj8ejzMzM1p4O7iCcW2gpnFu3B97OAAAARnglAgAAGCEiAACAESICAAAYISIAAIARIgIAABghIgAAgBEiAgAAGCEiAACAkf8DTvhAIlfDe6QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#visualizing distribution types in training set\n",
        "labels = list(map(lambda x: int(x[-1]),train_labels))\n",
        "#extracting X from Type_X string and converting into int\n",
        "plt.figure(figsize = (6,5))\n",
        "plt.bar(['Type 1','Type 2', 'Type 3'],[labels.count(1),labels.count(2),labels.count(3)])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl_XkTnoXSaP"
      },
      "source": [
        "# MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "stVjbOotWzci",
        "outputId": "45572fa2-b209-41db-c53a-774d05f8ff4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#initializing pretrained vgg model base\n",
        "'''vgg16 is a pre-built nn esp for img classification tasks'''\n",
        "from keras.applications.vgg16 import VGG16\n",
        "#its trained on imagenet(HUGE DATASET) it's already learned to recognize wide variety of features in images\n",
        "#include_top = false because we wanna use it as feature extractor and not sth which makes predictions\n",
        "conv_base = VGG16(weights = 'imagenet',include_top = False,input_shape = (180,180,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gOeqRHHWb629",
        "outputId": "6b180808-570c-4736-bff5-46f7ad3c78b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the number of trainable weights before freezing layers in the conv base: 26\n"
          ]
        }
      ],
      "source": [
        "# initial trainable layers before freezing\n",
        "print('This is the number of trainable weights '\n",
        "'before freezing layers in the conv base:', len(conv_base.trainable_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dpZm3s14cvGK"
      },
      "outputs": [],
      "source": [
        "#freezing few layers so it doesn't get too focused on just our task but also\n",
        "#recognizes basic stuffs like edges,colors,simple shapes so to not mess up this we freeze lower layers\n",
        "for layer in conv_base.layers[:-5]: #all layers freezed except last 5\n",
        "  layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JrP1kmH_dXKY",
        "outputId": "56a4f165-fb6a-4853-aa56-3088c061932e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the number of trainable weights after freezing layers in the conv base: 6\n"
          ]
        }
      ],
      "source": [
        "# total trainable layers after freezing\n",
        "print('This is the number of trainable weights '\n",
        "'after freezing layers in the conv base:', len(conv_base.trainable_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GdDXMIWudiM8"
      },
      "outputs": [],
      "source": [
        "#building model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten , Dropout\n",
        "import keras\n",
        "METRICS = [\n",
        "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall'),\n",
        "    keras.metrics.AUC(name='prc',curve='PR'),\n",
        "]\n",
        "model = Sequential([conv_base,\n",
        "                    Flatten(),\n",
        "                    Dropout(0.5),\n",
        "                    Dense(3,activation='softmax')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aAPatt07glga"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision\n",
        "    Only computes a batch-wise average of precision.\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tUDCcHMJ2hzt",
        "outputId": "42271460-b6fd-4de8-81f0-9180b696fefb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_cfdBLh-0aLU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(\n",
        "    optimizer= Adam(0.0001),\n",
        "    loss= 'sparse_categorical_crossentropy',\n",
        "    metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cYMGz_W_2nrp",
        "outputId": "8c4b3d1e-0aff-4199-a36c-be1bc25f719c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1094, 136)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TRAIN_STEPS = len(train_labels)//BATCH_SIZE\n",
        "VAL_STEPS = len(val_labels)//BATCH_SIZE\n",
        "TRAIN_STEPS,VAL_STEPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3n_BTTX93ALq"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "# initialize callbacks\n",
        "reduceLR = ReduceLROnPlateau(  #if val loss stops improving after certain epochs(givenby patience) lr reduced by factor0.2\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose= 1,\n",
        "    mode='min',\n",
        "    factor=  0.2,\n",
        "    min_lr = 1e-5)\n",
        "\n",
        "# early_stopping = EarlyStopping(  #if model's accuracy doesn't improve for a while, training stopped\n",
        "#     monitor='val_accuracy',\n",
        "#     patience = 10,\n",
        "#     verbose=1,\n",
        "#     mode='max',\n",
        "#     restore_best_weights= True)  # model's weights restored to best performing config\n",
        "\n",
        "# checkpoint = ModelCheckpoint(\n",
        "#     'cervicalModel_weights.hdf5',   # saves weight whenever val_acc improves,also saves the BEST ONE\n",
        "#     monitor='val_accuracy',\n",
        "#     verbose=1,\n",
        "#     save_best_only=True,\n",
        "#     mode= 'max')\n",
        "filepath = \"/content/drive/MyDrive/cervix/cervical-weights/nemodels.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=False, mode='max')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox2mwKnjANh_"
      },
      "source": [
        "# TRAIN MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDBxpL9uANHN",
        "outputId": "a481111a-62db-4178-8090-82242a19be06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "821/821 [==============================] - ETA: 0s - loss: 1.0258 - accuracy: 0.5071\n",
            "Epoch 1: saving model to /content/drive/MyDrive/cervix/cervical-weights/nemodels.hdf5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "821/821 [==============================] - 3703s 5s/step - loss: 1.0258 - accuracy: 0.5071 - val_loss: 0.9685 - val_accuracy: 0.5294 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.9848 - accuracy: 0.5267\n",
            "Epoch 2: saving model to /content/drive/MyDrive/cervix/cervical-weights/nemodels.hdf5\n",
            "821/821 [==============================] - 3682s 4s/step - loss: 0.9848 - accuracy: 0.5267 - val_loss: 0.9563 - val_accuracy: 0.5392 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "393/821 [=============>................] - ETA: 28:46 - loss: 0.9746 - accuracy: 0.5247"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch = TRAIN_STEPS,\n",
        "    validation_data = val_gen,\n",
        "    validation_steps = VAL_STEPS,\n",
        "    epochs = 100,\n",
        "    callbacks = [checkpoint,reduceLR]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoiA8c5sOcN-"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/cervix/traineddata.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}